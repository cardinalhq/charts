# Default values for lakerunner
# This is a YAML-formatted file.

# Global settings
global:
  # The names of image pull secrets to use for all images.
  # Optional.
  imagePullSecrets: []
  # Additional annotations to add to all resources.
  # This is a key: value map.
  # Optional.
  annotations: {}
  # Additional labels to add to all resources.
  # This is a key: value map.
  # Optional.
  labels: {}
  # Common environment variables to inject into all containers.
  # This is a key: value map.
  # Optional.
  env:
    # Enable OpenTelemetry logs, metrics, and traces for all components.
    # Set to "true" to enable, "false" to disable.
    - name: ENABLE_OTLP_TELEMETRY
      value: "false"
    # If you want to set up the endpoint where it should send, do that here as well:
    # - name: OTEL_EXPORTER_OTLP_ENDPOINT
    #   value: "http://${HOST_IP}:4317"
    # - name: HOST_IP
    #   valueFrom:
    #     fieldRef:
    #       fieldPath: status.hostIP
    # - name: ENVAR1
    #   value: "value1"
    # - name: ENVAR2
    #   value: "value2"
  # A node selector to apply to all pods.
  # This is standard Kubernetes node selector syntax.
  # Optional.
  nodeSelector: {}
    # node-role.kubernetes.io/worker: ""
    # spot-instance: "true"
  # A set of tolerations to apply to all pods.
  # This is standard Kubernetes toleration syntax.
  # Optional.
  tolerations: {}
    # - key: "spot"
    #   operator: "Equal"
    #   value: "true"
    #   effect: "NoSchedule"
  # A set of affinity rules to apply to all pods.
  # This is standard Kubernetes affinity syntax.
  # Optional.
  affinity: {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #       - matchExpressions:
    #           - key: "node-role.kubernetes.io/worker"
    #             operator: In
    #             values:
    #               - "true"
  # The default service account to use for all pods.
  # This service account is assigned some namespaced permissions.
  # Additional cloud-provider specific permissions can be added
  # to this service account, either by annotations or through
  # systems like EKS's IAM roles for service accounts (IRSA).
  # Required.

serviceAccount:
  # Create indicates whether to create the service account.
  # If set to false, the service account must already
  # exist in the Kubernetes cluster.
  create: true
  # The name of the service account to use.
  # If `create` is set to true, this will be the name of the service
  # account created by the Helm chart with the format `<release-name>-lakerunner`.
  # Required.
  name: "lakerunner"
  # The annotations to add to the service account.
  # Only added if create is set to true.
  # This is a key: value map.
  # Optional.
  annotations: {}

# Set the AWS region and credentials for the LakeRunner deployment.
# By default, the credentials are not created, and expected to be in an existing Kubernetes secret
# with the keys `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
#
# This is a limitation in the QueryAPI that we cannot currently pull this in through any other
# means, as the DuckDB driver requires these to be actual keys.
aws:
  # AWS region for the LakeRunner deployment.
  # Required.
  region: "us-east-2"  # AWS region for the deployment

  # Name of the Kubernetes secret that contains the AWS credentials.
  # This secret should contain the keys `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
  # If `aws.create` is set to true, this secret will be created by the Helm chart.
  # If `aws.create` is set to false, this secret must already exist in the Kubernetes cluster.
  # The secret will have the format `<release-name>-aws-credentials`.
  # If you want to use a different name, you can set it here.
  # Required.
  secretName: "aws-credentials"

  # Whether to create the AWS credentials secret. If set to true, the secret will be created
  # with the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the values below.
  # If set to false, the secret must already exist in the Kubernetes cluster.
  create: false

  # Only the data query pods need to have the actual credentials injected if
  # credentials are provivided in some other manner, such as using EKS's
  # IAM roles for service accounts (IRSA).  If you are using IRSA, set this to false.
  # Note that regardless of this setting, the credentials will be injected into the
  # data-query pods, and so the secret key and key ID must be set.
  inject: true

  # AWS credentials. If `aws.create` is set to true, these will be created in the
  # Kubernetes secret specified by `aws.secretName`. If `aws.create` is false, these
  # must be set in the Kubernetes secret specified by `aws.secretName`.
  # Required if `aws.create` is true.
  AWS_SECRET_ACCESS_KEY: ""
  AWS_ACCESS_KEY_ID: ""

# Storage profile configuration
# At least one storage profile is required for the LakeRunner to function.
# Required.
storageProfiles:
  source: config # only value for general use.
  configmapName: "storage-profiles"  # Name of the ConfigMap containing storage profiles
  create: true
  yaml: []
    # - organization_id: dddddddd-aaaa-4ff9-ae8f-365873c552f0
    #   instance_num: 1
    #   collector_name: "kubepi"
    #   cloud_provider: "aws"
    #   region: "us-east-2"
    #   bucket: "datalake-11ndajkhk"
    #   use_path_style: true

# API keys are used to control access to the data lake through the data-api service.
# Each key must be unique, and it associates an API key with an organization ID.
# An example format is shown below.  If you wish to create the secret outside of the
# helm chart, follow this format, set `apiKeys.create` to `false`, and create a Kubernetes secret
# with the content in the secret under a key named `apikeys.yaml`.
# Required.
apiKeys:
  source: config # only value for general use.
  secretName: "apikeys" # will have the format <release-name>-apikeys once deployed
  create: true
  yaml: []
    # - organization_id: dddddddd-aaaa-4ff9-ae8f-365873c552f0
    #   keys:
    #     - my-api-key-1
    #     - my-api-key-2

# Database configuration
# Required.
database:
  secretName: "pg-credentials"  # Name of the Kubernetes secret containing database credentials
  create: true  # Whether to create the database credentials secret
  # LRDB (LakeRunner Database) - PostgreSQL
  lrdb:
    # PostgreSQL hostname.
    # Required.
    host: ""
    # PostgreSQL port.  Default is 5432.
    # Required.
    port: 5432
    # PostgreSQL database name.
    # Required.
    name: "lakerunner"
    # PostgreSQL username.
    # Required.
    username: "lakerunner"
    # PostgreSQL password.
    # Optional, but recommended.
    password: ""
    # SSL mode for PostgreSQL connection.  Default is "require".
    # Options are "disable", "allow", "prefer", "require", "verify-ca", and "verify-full".
    # See https://www.postgresql.org/docs/current/libpq-ssl.html#LIBPQ-SSL-SSLMODE-STATEMENTS for more details.
    # Optional, but recommended.
    sslMode: "require"

# Authentication configuration
auth:
  # token is used for the query-api pod to authenticate to the query-worker pods.
  # This secret must contain a key named `TOKEN` with the value being the token string.
  # Required.
  token:
    # The name of the Kubernetes secret that contains the token.
    # Required.
    secretName: "query-token"
    # If create is set to true, the secret will be created with the secretValue provided below.
    create: true
    secretValue: ""

# Setup job configuration (runs before all other services)
# This job is responsible for running database migrations and initial setup tasks.
setup:
  enabled: true
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: "latest"
    pullPolicy: Always
  resources:
    requests:
      cpu: 1100m
      memory: 250Mi
    limits:
      cpu: 1100m
      memory: 250Mi
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Ingest Logs configuration
ingestLogs:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 2
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: "latest"
    pullPolicy: Always
  resources:
    requests:
      cpu: 500m
      memory: 200Mi
    limits:
      cpu: 1100m
      memory: 200Mi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Ingest Metrics configuration
ingestMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 2
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: "latest"
    pullPolicy: Always
  resources:
    requests:
      cpu: 1100m
      memory: 500Mi
    limits:
      cpu: 1100m
      memory: 500Mi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Compact Logs configuration
compactLogs:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: "latest"
    pullPolicy: Always
  resources:
    requests:
      cpu: 1100m
      memory: 500Mi
    limits:
      cpu: 1100m
      memory: 500Mi
  temporaryStorage:
    size: "5Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Compact Metrics configuration
compactMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: "latest"
    pullPolicy: Always
  resources:
    requests:
      cpu: 2100m
      memory: 1Gi
    limits:
      cpu: 2100m
      memory: 1Gi
  temporaryStorage:
    size: "5Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Rollup Metrics configuration
rollupMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: "latest"
    pullPolicy: Always
  resources:
    requests:
      cpu: 1100m
      memory: 1Gi
    limits:
      cpu: 1100m
      memory: 1Gi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Sweeper configuration
sweeper:
  enabled: true
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: "latest"
    pullPolicy: Always
  resources:
    requests:
      cpu: 100m
      memory: 80Mi
    limits:
      cpu: 250m
      memory: 80Mi
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# PubSub configuration
pubsub:
  HTTP:
    enabled: false
    replicas: 2 # recommend at least 2
    service:
      type: ClusterIP
    image:
      repository: public.ecr.aws/cardinalhq.io/lakerunner
      tag: "latest"
      pullPolicy: Always
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 200Mi
    nodeSelector: {}
    tolerations: {}
    affinity: {}
  SQS:
    enabled: true
    replicas: 1
    queueURL: "https://sqs.us-east-2.amazonaws.com/123456789012/my-queue"
    region: "us-east-2" # should match the region of the SQS queue
    roleARN: ""
    image:
      repository: public.ecr.aws/cardinalhq.io/lakerunner
      tag: "latest"
      pullPolicy: Always
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 200Mi
    nodeSelector: {}
    tolerations: {}
    affinity: {}

# Query API configuration
queryApi:
  enabled: true
  replicas: 1
  minWorkers: 2
  maxWorkers: 4
  service:
    type: ClusterIP
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner/query-api
    tag: "latest"
    pullPolicy: Always
  resources:
    requests:
      cpu: 2000m
      memory: 8Gi
    limits:
      cpu: 2000m
      memory: 8Gi
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Query Worker configuration
#
# We do not specify a number of replicas here, as the QueryAPI will scale the number of workers
# based on the number of queries being processed.
queryWorker:
  enabled: true
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner/query-worker
    tag: "latest"
    pullPolicy: Always
  resources:
    requests:
      cpu: 3500m
      memory: 12Gi
    limits:
      cpu: 3500m
      memory: 12Gi
  service:
    type: ClusterIP
  temporaryStorage:
    size: "16Gi"
  nodeSelector: {}
  tolerations: {}
  affinity: {}
