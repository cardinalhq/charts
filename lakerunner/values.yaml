# Default values for lakerunner
# This is a YAML-formatted file.

# Global settings
global:
  # Global resource configuration
  resources:
    # Whether to render resource limits and requests in deployments
    # Set to false to omit resource specifications entirely from rendered templates
    # Default: true
    enabled: true
  # CardinalHQ telemetry configuration
  cardinal:
    # API key for CardinalHQ telemetry endpoint
    # If provided, telemetry will be sent to CardinalHQ's controlled endpoint
    # If not provided, no telemetry endpoint will be configured
    apiKey: ""
  # The names of image pull secrets to use for all images.
  # Optional.
  imagePullSecrets: []
  # Global image settings
  image:
    # Default repository for all lakerunner images
    # Override this to use a different registry/repository for all images
    # Example: "public.ecr.aws/cardinalhq.io/lakerunner"
    repository: ""
    # Default tag for all lakerunner images (defaults to Chart.appVersion)
    # Override this to use a different tag for all images
    tag: ""
    # Default pull policy for all images
    pullPolicy: "IfNotPresent"
  # Global temporary storage configuration
  # This allows switching between emptyDir and ephemeral volume claim templates
  # Storage sizes are configured per-service in their respective temporaryStorage.size settings
  temporaryStorage:
    # Type of temporary storage: "emptyDir" or "ephemeral"
    # - emptyDir: Traditional emptyDir volumes (default)
    # - ephemeral: Uses ephemeral volume claim templates for CSI-based storage
    type: "emptyDir"
    # Configuration for ephemeral volume claim templates (only used when type: "ephemeral")
    ephemeral:
      # Storage class to use for ephemeral volumes (leave empty for default storage class)
      storageClassName: ""
      # Additional labels to apply to ephemeral volume claim templates
      labels:
        {}
        # type: my-frontend-volume
  # Global autoscaling configuration
  autoscaling:
    # Global scaling mode for all components (can be overridden per component)
    # Options: "keda" (work queue-based), "disabled"
    #
    # PRODUCTION: Use "keda" for production environments.
    # Work queue-based scaling (KEDA) provides intelligent scaling based on actual
    # workload backlog, which is essential for micro-batch workloads.
    #
    # You must have KEDA installed in your cluster to use "keda" mode.
    # See https://keda.sh for installation instructions.
    mode: "disabled"
  # Global health probe configuration
  # These defaults can be overridden per service
  healthProbes:
    # Global default for health probe enablement
    enabled: false # Set to true to enable health probes globally
    # Default health check port name (most services use this)
    healthcheckPort: 8090
  # Additional annotations to add to all resources.
  # This is a key: value map.
  # Optional.
  annotations: {}
  # Additional labels to add to all resources.
  # This is a key: value map.
  # Optional.
  labels: {}
  # Common environment variables to inject into all containers.
  # This is a key: value map.
  # Optional.
  env:
    # - name: ENVAR1
    #   value: "value1"
    # - name: ENVAR2
    #   value: "value2"
  # A node selector to apply to all pods.
  # This is standard Kubernetes node selector syntax.
  # Optional.
  nodeSelector:
    {}
    # node-role.kubernetes.io/worker: ""
    # spot-instance: "true"
  # A set of tolerations to apply to all pods.
  # This is standard Kubernetes toleration syntax.
  # Optional.
  tolerations:
    []
    # - key: "spot"
    #   operator: "Equal"
    #   value: "true"
    #   effect: "NoSchedule"
  # A set of affinity rules to apply to all pods.
  # This is standard Kubernetes affinity syntax.
  # Optional.
  affinity:
    {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #       - matchExpressions:
    #           - key: "node-role.kubernetes.io/worker"
    #             operator: In
    #             values:
    #               - "true"
  # The default service account to use for all pods.
  # This service account is assigned some namespaced permissions.
  # Additional cloud-provider specific permissions can be added
  # to this service account, either by annotations or through
  # systems like EKS's IAM roles for service accounts (IRSA).
  # Required.

serviceAccount:
  # Create indicates whether to create the service account.
  # If set to false, the service account must already
  # exist in the Kubernetes cluster.
  create: true
  # The name of the service account to use.
  # If `create` is set to true, this will be the name of the service
  # account created by the Helm chart with the format `<release-name>-lakerunner`.
  # Required.
  name: "lakerunner"
  # The annotations to add to the service account.
  # Only added if create is set to true.
  # This is a key: value map.
  # Optional.
  annotations: {}

# Cloud provider configuration for storage operations
# This section configures authentication for the cloud storage backend used by LakeRunner.
# Choose the primary cloud provider and configure credentials accordingly.
cloudProvider:
  # Primary cloud provider for storage operations
  # Options: "aws", "gcp", "azure"
  # This determines which credentials are injected into all LakeRunner pods.
  # Required.
  provider: "aws" # Change to "azure" or "gcp" based on your storage profiles

  # AWS configuration (used when provider: "aws")
  aws:
    # AWS region for the deployment
    # Required when using AWS provider
    region: ""

    # Name of the Kubernetes secret that contains AWS credentials
    # Secret should contain: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
    secretName: "aws-credentials"

    # Whether to create the AWS credentials secret
    create: false

    # Whether to inject AWS credentials into pods
    # Set to false if using IAM roles (IRSA) or EC2 instance profiles
    inject: true

    # AWS credentials (only used if create: true)
    accessKeyId: ""
    secretAccessKey: ""

  # Azure configuration (used when provider: "azure")
  azure:
    # Name of the Kubernetes secret that contains Azure credentials
    # Secret contents vary based on authType
    secretName: "azure-credentials"

    # Whether to create the Azure credentials secret
    create: false

    # Whether to inject Azure credentials into pods
    # Set to false if using system managed identity without explicit configuration
    inject: true

    # Azure authentication type
    # Options: "service_principal", "user_managed_identity", "system_managed_identity",
    #          "workload_identity", "connection_string"
    authType: "service_principal"

    # Azure service principal credentials (only used if authType: "service_principal")
    clientId: ""
    clientSecret: ""
    tenantId: ""

    # Connection string (only used if authType: "connection_string")
    connectionString: ""

    # Federated token file path (only used if authType: "workload_identity")
    # Default path for Azure Workload Identity webhook
    federatedTokenFile: "/var/run/secrets/azure/tokens/azure-identity-token"

  # GCP configuration (used when provider: "gcp")
  gcp:
    # Name of the Kubernetes secret that contains GCP credentials
    secretName: "gcp-credentials"

    # Whether to create the GCP credentials secret
    create: false

    # Whether to inject GCP credentials into pods
    # Set to false if using Workload Identity or other GCP auth methods
    inject: true

    # GCP credentials (only used if create: true)
    # Service account JSON for native GCP API access (e.g., Pub/Sub, GCS)
    serviceAccountJson: ""

# Kafka connection settings
kafka:
  # Name of the Kubernetes secret containing Kafka credentials
  secretName: "kafka-credentials"
  # Whether to create the Kafka credentials secret
  create: true
  # Secret key names (only needed if using an existing secret with different key names)
  usernameKey: "KAFKA_USERNAME" # Key name for username in the secret
  passwordKey: "KAFKA_PASSWORD" # Key name for password in the secret
  # Kafka broker configuration
  # Required - Comma-separated list of Kafka broker addresses
  # Example: "b-1.cluster.kafka.us-east-1.amazonaws.com:9096,b-2.cluster.kafka.us-east-1.amazonaws.com:9096"
  brokers: ""
  # SASL (Simple Authentication and Security Layer) configuration
  sasl:
    # Whether to enable SASL authentication (default: true)
    enabled: true
    # SASL mechanism - Options: "PLAIN", "SCRAM-SHA-256", "SCRAM-SHA-512"
    mechanism: "SCRAM-SHA-512"
    # SASL username (only used if create: true)
    username: ""
    # SASL password (only used if create: true)
    password: ""
  # TLS configuration
  tls:
    # Whether to enable TLS encryption (default: true)
    enabled: true

# Kafka topics configuration
# Defines the Kafka topics that will be created during setup.
kafkaTopics:
  configmapName: "kafka-topics" # Name of the ConfigMap containing Kafka topics
  # IF create is true, the contents of the `config` block will be used to set up
  # the Kafka topics.  If false, the configuration defined below will be
  # ignored, but
  create: true
  config:
    version: 2
    defaults:
      # This will be the default for all topics unless overridden below.
      # Can only increase this value as Kafka does not support decreasing
      # partitions.
      partitionCount: 16
      # Set the replication factor for all topics.  This can be changed
      # and the new value will be configured on the next setup job run.
      replicationFactor: 2
      # Additional topic-level configuration options.
      # These will be applied to all topics unless overridden below,
      # on the next setup job run.
      options:
        "retention.ms": "604800000" # 7 days
    # Worker overrides by service type will configure the
    # topic that feeds these services.  See the docs for more details.
    # workers:
    #   objstore.ingest.logs:
    #     partitionCount: 16
    #   objstore.ingest.metrics:
    #     partitionCount: 16
    #   objstore.ingest.traces:
    #     partitionCount: 16
    #   boxer.logs.compact:
    #     partitionCount: 16
    #   boxer.metrics.compact:
    #     partitionCount: 16
    #   boxer.traces.compact:
    #     partitionCount: 16
    #   boxer.metrics.rollup:
    #     partitionCount: 16

# Storage profile configuration
# At least one storage profile is required for the LakeRunner to function.
# Required.
# If you want to auto create the Cardinal Collector, make sure that your storage profile has the following fields:
# organization_id, collector_name, cloud_provider, region, bucket
storageProfiles:
  source: config # only value for general use.
  configmapName: "storage-profiles" # Name of the ConfigMap containing storage profiles
  create: true
  yaml:
    []
    # - organization_id: dddddddd-aaaa-4ff9-ae8f-365873c552f0
    #   instance_num: 1
    #   collector_name: "kubepi"
    #   cloud_provider: "aws"
    #   region: "us-east-2"
    #   bucket: "datalake-11ndajkhk"
    #   endpoint: ""
    #   use_path_style: true

# API keys are used to control access to the data lake through the data-api service.
# Each key must be unique, and it associates an API key with an organization ID.
# An example format is shown below.  If you wish to create the secret outside of the
# helm chart, follow this format, set `apiKeys.create` to `false`, and create a Kubernetes secret
# with the content in the secret under a key named `apikeys.yaml`.
# Required.
apiKeys:
  source: config # don't change as no other source is supported.
  secretName: "apikeys" # will have the format <release-name>-apikeys once deployed
  create: true
  yaml:
    []
    # - organization_id: dddddddd-aaaa-4ff9-ae8f-365873c552f0
    #   keys:
    #     - my-api-key-1
    #     - my-api-key-2

# Database configuration
# Required.
database:
  secretName: "pg-credentials" # Name of the Kubernetes secret containing database credentials
  create: true # Whether to create the database credentials secret
  # Secret key names (only needed if using an existing secret with different key names)
  passwordKey: "LRDB_PASSWORD" # Key name for password in the secret
  # LRDB (LakeRunner Database) - PostgreSQL
  lrdb:
    # PostgreSQL hostname.
    # Required.
    host: ""
    # PostgreSQL port.  Default is 5432.
    # Required.
    port: 5432
    # PostgreSQL database name.
    # Required.
    name: "lakerunner"
    # PostgreSQL username.
    # Required.
    username: "lakerunner"
    # PostgreSQL password.
    # Optional, but recommended if not using an existing secret.
    password: ""
    # SSL mode for PostgreSQL connection.  Default is "require".
    # Options are "disable", "allow", "prefer", "require", "verify-ca", and "verify-full".
    # See https://www.postgresql.org/docs/current/libpq-ssl.html#LIBPQ-SSL-SSLMODE-STATEMENTS for more details.
    # Optional, but recommended.
    sslMode: "require"

# Configuration database settings
configdb:
  secretName: "configdb-credentials" # Name of the Kubernetes secret containing configdb credentials
  create: true # Whether to create the configdb credentials secret
  # Secret key names (only needed if using an existing secret with different key names)
  passwordKey: "CONFIGDB_PASSWORD" # Key name for password in the secret
  # CONFIGDB (Configuration Database) - PostgreSQL
  lrdb:
    # PostgreSQL hostname.
    # Required.
    host: ""
    # PostgreSQL port.  Default is 5432.
    # Required.
    port: 5432
    # PostgreSQL database name.
    # Required.
    name: "config"
    # PostgreSQL username.
    # Required.
    username: "config"
    # PostgreSQL password.
    # Optional, but recommended if not using an existing secret.
    password: ""
    # SSL mode for PostgreSQL connection.  Default is "require".
    # Options are "disable", "allow", "prefer", "require", "verify-ca", and "verify-full".
    # See https://www.postgresql.org/docs/current/libpq-ssl.html#LIBPQ-SSL-SSLMODE-STATEMENTS for more details.
    # Optional, but recommended.
    sslMode: "require"

# Setup job configuration (runs before all other services)
# This job is responsible for running database migrations and initial setup tasks.
setup:
  enabled: true
  replicas: 1
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 1100m
      memory: 250Mi
    limits:
      cpu: 1100m
      memory: 250Mi
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Kafka topic setup configuration
  # This enables the setup job to create Kafka topics from the kafkaTopics configuration
  kafka:
    enabled: true

# Ingest Logs configuration
ingestLogs:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: "1"
      memory: 4Gi
    limits:
      cpu: "1"
      memory: 4Gi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 30 # How often to check the queue (seconds)
      cooldownPeriod: 300 # Wait time before scaling down (seconds)
      targetQueueDepth: 100 # DEPRECATED: not used - controlled by monitor (external scaler)
      activationQueueDepth: 10 # DEPRECATED: not used - controlled by monitor (external scaler)
      cpuTrigger:
        enabled: true # Enable CPU-based scaling alongside queue depth
        targetUtilization: 80 # Target average CPU utilization percentage
  terminationGracePeriodSeconds: 600
  # Health probe configuration (inherits from global.healthProbes if not specified)
  healthProbes:
    enabled: null # null = inherit from global.healthProbes.enabled
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Ingest Metrics configuration
ingestMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: "1"
      memory: 4Gi
    limits:
      cpu: "1"
      memory: 4Gi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 30 # How often to check the queue (seconds)
      cooldownPeriod: 300 # Wait time before scaling down (seconds)
      targetQueueDepth: 150 # DEPRECATED: not used - controlled by monitor (external scaler)
      activationQueueDepth: 20 # DEPRECATED: not used - controlled by monitor (external scaler)
      cpuTrigger:
        enabled: true # Enable CPU-based scaling alongside queue depth
        targetUtilization: 80 # Target average CPU utilization percentage
  terminationGracePeriodSeconds: 600
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Ingest Traces configuration
ingestTraces:
  enabled: false
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: "1"
      memory: 4Gi
    limits:
      cpu: "1"
      memory: 4Gi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 30 # How often to check the queue (seconds)
      cooldownPeriod: 300 # Wait time before scaling down (seconds)
      targetQueueDepth: 120 # DEPRECATED: not used - controlled by monitor (external scaler)
      activationQueueDepth: 15 # DEPRECATED: not used - controlled by monitor (external scaler)
      cpuTrigger:
        enabled: true # Enable CPU-based scaling alongside queue depth
        targetUtilization: 80 # Target average CPU utilization percentage
  terminationGracePeriodSeconds: 600
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Compact Logs configuration
compactLogs:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: "1"
      memory: 4Gi
    limits:
      cpu: "1"
      memory: 4Gi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 60 # Longer interval for batch work
      cooldownPeriod: 600 # 10-minute cooldown for compaction
      targetQueueDepth: 10 # DEPRECATED: not used - controlled by monitor (external scaler)
      activationQueueDepth: 1 # DEPRECATED: not used - controlled by monitor (external scaler)
      cpuTrigger:
        enabled: true # Enable CPU-based scaling alongside queue depth
        targetUtilization: 80 # Target average CPU utilization percentage
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Compact Metrics configuration
compactMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: "1"
      memory: 5Gi
    limits:
      cpu: "1"
      memory: 5Gi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 60 # Longer interval for batch work
      cooldownPeriod: 600 # 10-minute cooldown for compaction
      targetQueueDepth: 10 # DEPRECATED: not used - controlled by monitor (external scaler)
      activationQueueDepth: 1 # DEPRECATED: not used - controlled by monitor (external scaler)
      cpuTrigger:
        enabled: true # Enable CPU-based scaling alongside queue depth
        targetUtilization: 80 # Target average CPU utilization percentage
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Compact Traces configuration
compactTraces:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: "1"
      memory: 4Gi
    limits:
      cpu: "1"
      memory: 4Gi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 60 # Longer interval for batch work
      cooldownPeriod: 600 # 10-minute cooldown for compaction
      targetQueueDepth: 10 # DEPRECATED: not used - controlled by monitor (external scaler)
      activationQueueDepth: 1 # DEPRECATED: not used - controlled by monitor (external scaler)
      cpuTrigger:
        enabled: true # Enable CPU-based scaling alongside queue depth
        targetUtilization: 80 # Target average CPU utilization percentage
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Rollup Metrics configuration
rollupMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: "4"
      memory: 8Gi
    limits:
      cpu: "4"
      memory: 8Gi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 60 # Longer interval for batch work
      cooldownPeriod: 600 # 10-minute cooldown for rollup processing
      targetQueueDepth: 15 # DEPRECATED: not used - controlled by monitor (external scaler)
      activationQueueDepth: 2 # DEPRECATED: not used - controlled by monitor (external scaler)
      cpuTrigger:
        enabled: true # Enable CPU-based scaling alongside queue depth
        targetUtilization: 80 # Target average CPU utilization percentage
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Boxer configuration
# New instances-based configuration for running multiple tasks in a single pod
boxers:
  # Default configuration that applies to all boxer instances
  # The number of replicas to run. This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: "200m"
      memory: 250Mi
    limits:
      cpu: "200m"
      memory: 250Mi
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 1
    keda:
      pollingInterval: 60 # Longer interval for batch work
      cooldownPeriod: 600 # 10-minute cooldown for processing
      targetQueueDepth: 15 # DEPRECATED: not used - controlled by monitor (external scaler)
      activationQueueDepth: 2 # DEPRECATED: not used - controlled by monitor (external scaler)
      cpuTrigger:
        enabled: true # Enable CPU-based scaling alongside queue depth
        targetUtilization: 80 # Target average CPU utilization percentage
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Boxer instances configuration
  instances:
    # Default configuration: single boxer with all tasks
    - name: common
      tasks:
        - compact-logs
        - compact-metrics
        - compact-traces
        - ingest-logs
        - ingest-metrics
        - ingest-traces
        - rollup-metrics

# Sweeper configuration
sweeper:
  enabled: true
  replicas: 1
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 250m
      memory: 300Mi
    limits:
      cpu: 250m
      memory: 300Mi
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Monitoring service configuration
monitoring:
  enabled: true
  replicas: 1
  grpcPort: 9090
  service:
    type: ClusterIP
    annotations: {}
    nodePort: null
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 250m
      memory: 100Mi
    limits:
      cpu: 250m
      memory: 100Mi
  terminationGracePeriodSeconds: 30
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Admin API configuration
adminApi:
  enabled: true
  replicas: 1
  grpcPort: 9091
  service:
    type: ClusterIP
    annotations: {}
    nodePort: null
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 250m
      memory: 200Mi
    limits:
      cpu: 250m
      memory: 200Mi
  terminationGracePeriodSeconds: 30
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# PubSub configuration
pubsub:
  HTTP:
    enabled: false
    replicas: 2 # recommend at least 2 in production
    service:
      type: ClusterIP
      port: 8080
      annotations: {}
      nodePort: null
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
    image:
      repository: ghcr.io/cardinalhq/lakerunner
      tag: ""
      pullPolicy: ""
    resources:
      requests:
        cpu: "1"
        memory: 512Mi
      limits:
        cpu: "1"
        memory: 512Mi
    terminationGracePeriodSeconds: 120
    nodeSelector: {}
    tolerations: []
    affinity: {}
  SQS:
    enabled: false
    replicas: 2
    # REQUIRED when enabled: Your SQS queue URL
    # Example: "https://sqs.us-east-2.amazonaws.com/123456789012/my-queue"
    queueURL: ""
    # AWS region for the SQS queue - defaults to global aws.region if not specified
    # Example: "us-east-2"
    region: ""
    roleARN: ""
    image:
      repository: ghcr.io/cardinalhq/lakerunner
      tag: ""
      pullPolicy: ""
    resources:
      requests:
        cpu: "1"
        memory: 512Mi
      limits:
        cpu: "1"
        memory: 512Mi
    terminationGracePeriodSeconds: 120
    nodeSelector: {}
    tolerations: []
    affinity: {}
  GCP:
    enabled: false
    replicas: 1
    # REQUIRED when enabled: Your GCP project ID
    # Example: "my-project-123456"
    projectID: ""
    # REQUIRED when enabled: Your GCP Pub/Sub subscription ID
    # Example: "my-subscription"
    subscriptionID: ""
    image:
      repository: ghcr.io/cardinalhq/lakerunner
      tag: ""
      pullPolicy: ""
    resources:
      requests:
        cpu: "1"
        memory: 512Mi
      limits:
        cpu: "1"
        memory: 512Mi
    terminationGracePeriodSeconds: 120
    nodeSelector: {}
    tolerations: []
    affinity: {}
    env: []
  Azure:
    enabled: false
    replicas: 1
    # REQUIRED when enabled: Your Azure Storage Account name
    # Example: "mystorageaccount"
    storageAccount: ""
    # REQUIRED when enabled: Your Azure Storage Queue name
    # Example: "sqs"
    queueName: ""
    image:
      repository: ghcr.io/cardinalhq/lakerunner
      tag: ""
      pullPolicy: ""
    resources:
      requests:
        cpu: 200m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 200Mi
    terminationGracePeriodSeconds: 120
    nodeSelector: {}
    tolerations: []
    affinity: {}
    env: []

# Query API configuration
queryApi:
  enabled: true
  replicas: 2
  service:
    type: ClusterIP
    port: 8080
    annotations: {}
    nodePort: null
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: "1"
      memory: 4Gi
    limits:
      cpu: "1"
      memory: 4Gi
  terminationGracePeriodSeconds: 120
  nodeSelector: {}
  tolerations: []
  affinity: {}
  env: []

# Query Worker configuration
queryWorker:
  enabled: true
  replicas: 2
  image:
    repository: ghcr.io/cardinalhq/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: "2"
      memory: 4Gi
    limits:
      cpu: "2"
      memory: 4Gi
  service:
    type: ClusterIP
    port: 8081
    annotations: {}
    nodePort: null
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
  temporaryStorage:
    size: "100Gi"
  terminationGracePeriodSeconds: 120
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Grafana configuration
grafana:
  enabled: false
  replicas: 1 # For replicas > 1, configure external database via grafana.env (see Grafana docs)
  # CardinalHQ LakeRunner datasource configuration
  # This provides a simplified way to configure the Cardinal datasource
  # Required.
  cardinal:
    # REQUIRED: API key for datasource authentication
    # Must match a key from apiKeys.yaml configuration
    apiKey: ""
    # Optional: Custom endpoint URL for the Cardinal datasource
    # If not specified, auto-configured to point to the deployed query-api service
    # Example: "http://my-custom-query-api:8080"
    endpoint: ""
    # Optional: Custom name for the datasource (defaults to "Cardinal")
    name: "Cardinal Lakerunner"
    # Optional: Whether this datasource should be the default (defaults to true)
    isDefault: true
    # Optional: Whether this datasource should be editable in Grafana UI (defaults to true)
    editable: true
  image:
    repository: grafana/grafana
    tag: "latest"
    pullPolicy: ""
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 200m
      memory: 256Mi
  service:
    type: ClusterIP
    port: 3000
    annotations: {}
    nodePort: null
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
  # CardinalHQ LakeRunner datasource plugin configuration
  # This plugin is REQUIRED for LakeRunner to function properly
  cardinalPlugin:
    # URL to the CardinalHQ LakeRunner datasource plugin
    # air-gapped deployments should download the plugin and provide a local path
    url: "https://github.com/cardinalhq/cardinalhq-lakerunner-datasource/releases/download/v1.9.9/cardinalhq-lakerunner-datasource.zip;cardinalhq-lakerunner-datasource"
  # Additional optional Grafana plugins (semicolon-separated list)
  # These will be installed in addition to the required CardinalHQ plugin
  additionalPlugins: ""
  # Additional datasource configurations
  # Use this to add extra datasources beyond the Cardinal datasource
  # Each key becomes a filename in the datasources provisioning directory
  # Optional.
  datasources:
    {}
    # example-datasource.yaml:
    #   apiVersion: 1
    #   datasources:
    #     - name: Prometheus
    #       type: prometheus
    #       url: http://prometheus:9090

# Collector configuration
# This section controls the CardinalHQ Collector resource creation
collector:
  # Whether to create the Collector resource
  # Default: true (creates collector when Cardinal API key and storage profiles are configured)
  enabled: false

  # Resource configuration for the collector
  # CPU and memory limits and requests
  cpu: "1"
  memory: "1Gi"

  # Number of collector replicas
  replicas: 1

  # Additional environment variables to inject into the Collector
  # These will be merged with the default environment variables
  # Optional.
  env:
    # - name: CUSTOM_ENV_VAR
    #   value: "custom-value"
    # - name: ANOTHER_VAR
    #   value: "another-value"
    # - name: SECRET_VAR
    #   valueFrom:
    #     secretKeyRef:
    #       name: my-secret
    #       key: my-key

  # Additional labels to apply to the Collector resource
  # These will be merged with the default labels
  # Optional.
  labels: {}

  # Additional annotations to apply to the Collector resource
  # These will be merged with the default annotations
  # Optional.
  annotations: {}

# Debug container configuration
# A PostgreSQL debugging container with psql client for troubleshooting
debugger:
  # Whether to deploy the debug container
  enabled: false
  image:
    repository: postgres
    tag: "15-alpine"
    pullPolicy: ""
