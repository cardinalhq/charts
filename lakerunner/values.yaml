# Default values for lakerunner
# This is a YAML-formatted file.

# Global settings
global:
  # The names of image pull secrets to use for all images.
  # Optional.
  imagePullSecrets: []
  # Global image settings
  image:
    # Default tag for all lakerunner images (defaults to Chart.appVersion)
    # Override this to use a different tag for all images
    tag: ""
    # Default pull policy for all images
    pullPolicy: "Always"
  # Global autoscaling configuration
  autoscaling:
    # Global scaling mode for all components (can be overridden per component)
    # Options: "hpa" (CPU-based), "keda" (work queue-based), "disabled"
    # PRODUCTION RECOMMENDATION: Use "keda" for production environments.
    # CPU-based scaling (HPA) is insufficient for micro-batch workloads and may
    # cause poor performance. Work queue-based scaling (KEDA) provides intelligent
    # scaling based on actual workload backlog.
    mode: "hpa"
  # KEDA configuration (only used when scaling mode is "keda")
  # KEDA automatically creates and manages HPA resources - do not deploy
  # both KEDA ScaledObjects and manual HPA for the same workload.
  keda:
    enabled: false  # Global KEDA enable/disable
    # Authentication reference for PostgreSQL connection
    authenticationRef: "keda-postgresql-auth"
    # Alternative: use full connection string from environment variable
    connectionFromEnv: ""
  # Additional annotations to add to all resources.
  # This is a key: value map.
  # Optional.
  annotations: {}
  # Additional labels to add to all resources.
  # This is a key: value map.
  # Optional.
  labels: {}
  # Common environment variables to inject into all containers.
  # This is a key: value map.
  # Optional.
  env:
    # Enable OpenTelemetry logs, metrics, and traces for all components.
    # Set to "true" to enable, "false" to disable.
    - name: ENABLE_OTLP_TELEMETRY
      value: "false"
    # If you want to set up the endpoint where it should send, do that here as well:
    # - name: OTEL_EXPORTER_OTLP_ENDPOINT
    #   value: "http://${HOST_IP}:4317"
    # - name: HOST_IP
    #   valueFrom:
    #     fieldRef:
    #       fieldPath: status.hostIP
    # - name: ENVAR1
    #   value: "value1"
    # - name: ENVAR2
    #   value: "value2"
  # A node selector to apply to all pods.
  # This is standard Kubernetes node selector syntax.
  # Optional.
  nodeSelector:
    {}
    # node-role.kubernetes.io/worker: ""
    # spot-instance: "true"
  # A set of tolerations to apply to all pods.
  # This is standard Kubernetes toleration syntax.
  # Optional.
  tolerations:
    {}
    # - key: "spot"
    #   operator: "Equal"
    #   value: "true"
    #   effect: "NoSchedule"
  # A set of affinity rules to apply to all pods.
  # This is standard Kubernetes affinity syntax.
  # Optional.
  affinity:
    {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #       - matchExpressions:
    #           - key: "node-role.kubernetes.io/worker"
    #             operator: In
    #             values:
    #               - "true"
  # The default service account to use for all pods.
  # This service account is assigned some namespaced permissions.
  # Additional cloud-provider specific permissions can be added
  # to this service account, either by annotations or through
  # systems like EKS's IAM roles for service accounts (IRSA).
  # Required.

serviceAccount:
  # Create indicates whether to create the service account.
  # If set to false, the service account must already
  # exist in the Kubernetes cluster.
  create: true
  # The name of the service account to use.
  # If `create` is set to true, this will be the name of the service
  # account created by the Helm chart with the format `<release-name>-lakerunner`.
  # Required.
  name: "lakerunner"
  # The annotations to add to the service account.
  # Only added if create is set to true.
  # This is a key: value map.
  # Optional.
  annotations: {}

# Set the AWS region and credentials for the LakeRunner deployment.
# By default, the credentials are not created, and expected to be in an existing Kubernetes secret
# with the keys `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
#
# This is a limitation in the QueryAPI that we cannot currently pull this in through any other
# means, as the DuckDB driver requires these to be actual keys.
aws:
  # AWS region for the LakeRunner deployment.
  # Required.
  region: "us-east-2" # AWS region for the deployment

  # Name of the Kubernetes secret that contains the AWS credentials.
  # This secret should contain the keys `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
  # If `aws.create` is set to true, this secret will be created by the Helm chart.
  # If `aws.create` is set to false, this secret must already exist in the Kubernetes cluster.
  # The secret will have the format `<release-name>-aws-credentials`.
  # If you want to use a different name, you can set it here.
  # Required.
  secretName: "aws-credentials"

  # Whether to create the AWS credentials secret. If set to true, the secret will be created
  # with the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the values below.
  # If set to false, the secret must already exist in the Kubernetes cluster.
  create: false

  # Only the data query pods need to have the actual credentials injected if
  # credentials are provivided in some other manner, such as using EKS's
  # IAM roles for service accounts (IRSA).  If you are using IRSA, set this to false.
  # Note that regardless of this setting, the credentials will be injected into the
  # data-query pods, and so the secret key and key ID must be set.
  inject: true

  # AWS credentials. If `aws.create` is set to true, these will be created in the
  # Kubernetes secret specified by `aws.secretName`. If `aws.create` is false, these
  # must be set in the Kubernetes secret specified by `aws.secretName`.
  # Required if `aws.create` is true.
  AWS_SECRET_ACCESS_KEY: ""
  AWS_ACCESS_KEY_ID: ""

# Storage profile configuration
# At least one storage profile is required for the LakeRunner to function.
# Required.
storageProfiles:
  source: config # only value for general use.
  configmapName: "storage-profiles" # Name of the ConfigMap containing storage profiles
  create: true
  yaml:
    []
    # - organization_id: dddddddd-aaaa-4ff9-ae8f-365873c552f0
    #   instance_num: 1
    #   collector_name: "kubepi"
    #   cloud_provider: "aws"
    #   region: "us-east-2"
    #   bucket: "datalake-11ndajkhk"
    #   use_path_style: true

# API keys are used to control access to the data lake through the data-api service.
# Each key must be unique, and it associates an API key with an organization ID.
# An example format is shown below.  If you wish to create the secret outside of the
# helm chart, follow this format, set `apiKeys.create` to `false`, and create a Kubernetes secret
# with the content in the secret under a key named `apikeys.yaml`.
# Required.
apiKeys:
  source: config # only value for general use.
  secretName: "apikeys" # will have the format <release-name>-apikeys once deployed
  create: true
  yaml:
    []
    # - organization_id: dddddddd-aaaa-4ff9-ae8f-365873c552f0
    #   keys:
    #     - my-api-key-1
    #     - my-api-key-2

# Database configuration
# Required.
database:
  secretName: "pg-credentials" # Name of the Kubernetes secret containing database credentials
  create: true # Whether to create the database credentials secret
  # LRDB (LakeRunner Database) - PostgreSQL
  lrdb:
    # PostgreSQL hostname.
    # Required.
    host: ""
    # PostgreSQL port.  Default is 5432.
    # Required.
    port: 5432
    # PostgreSQL database name.
    # Required.
    name: "lakerunner"
    # PostgreSQL username.
    # Required.
    username: "lakerunner"
    # PostgreSQL password.
    # Optional, but recommended.
    password: ""
    # SSL mode for PostgreSQL connection.  Default is "require".
    # Options are "disable", "allow", "prefer", "require", "verify-ca", and "verify-full".
    # See https://www.postgresql.org/docs/current/libpq-ssl.html#LIBPQ-SSL-SSLMODE-STATEMENTS for more details.
    # Optional, but recommended.
    sslMode: "require"

# Authentication configuration
auth:
  # token is used for the query-api pod to authenticate to the query-worker pods.
  # This secret must contain a key named `TOKEN` with the value being the token string.
  # Required.
  token:
    # The name of the Kubernetes secret that contains the token.
    # Required.
    secretName: "query-token"
    # If create is set to true, the secret will be created with the secretValue provided below.
    create: true
    secretValue: ""

# Setup job configuration (runs before all other services)
# This job is responsible for running database migrations and initial setup tasks.
setup:
  enabled: true
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 1100m
      memory: 250Mi
    limits:
      cpu: 1100m
      memory: 250Mi
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Ingest Logs configuration
ingestLogs:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 2
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 500m
      memory: 200Mi
    limits:
      cpu: 1100m
      memory: 200Mi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    # Scaling mode for this component (overrides global.autoscaling.mode)
    # Options: "hpa", "keda", "disabled", or "" to use global setting
    mode: ""
    # HPA configuration
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    # KEDA configuration (work queue-based scaling)
    keda:
      pollingInterval: 30    # How often to check the queue (seconds)
      cooldownPeriod: 300    # Wait time before scaling down (seconds)
      minReplicaCount: 1     # Minimum replicas (can be 0)
      maxReplicaCount: 10    # Maximum replicas
      postgresql:
        # Ingest backlog query - counts unclaimed log entries
        query: "SELECT COALESCE(COUNT(*), 0) FROM inqueue WHERE telemetry_type='logs' AND claimed_at IS NULL"
        targetQueryValue: 100           # "It's getting deep!" threshold
        activationTargetQueryValue: 10  # Start scaling threshold
        metricType: "AverageValue"      # AverageValue or Value
  terminationGracePeriodSeconds: 600
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Ingest Metrics configuration
ingestMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 2
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 1100m
      memory: 500Mi
    limits:
      cpu: 1100m
      memory: 500Mi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    # Scaling mode for this component (overrides global.autoscaling.mode)
    # Options: "hpa", "keda", "disabled", or "" to use global setting
    mode: ""
    # HPA configuration
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    # KEDA configuration (work queue-based scaling)
    keda:
      pollingInterval: 30    # How often to check the queue (seconds)
      cooldownPeriod: 300    # Wait time before scaling down (seconds)
      minReplicaCount: 1     # Minimum replicas (can be 0)
      maxReplicaCount: 15    # Maximum replicas (higher for metrics)
      postgresql:
        # Ingest backlog query - counts unclaimed metric entries
        query: "SELECT COALESCE(COUNT(*), 0) FROM inqueue WHERE telemetry_type='metrics' AND claimed_at IS NULL"
        targetQueryValue: 150           # Higher threshold for metrics
        activationTargetQueryValue: 20  # Start scaling threshold
        metricType: "AverageValue"      # AverageValue or Value
  terminationGracePeriodSeconds: 600
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Compact Logs configuration
compactLogs:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 1100m
      memory: 500Mi
    limits:
      cpu: 1100m
      memory: 500Mi
  temporaryStorage:
    size: "5Gi"
  autoscaling:
    enabled: true
    # Scaling mode for this component (overrides global.autoscaling.mode)
    mode: ""
    # HPA configuration
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    # KEDA configuration (work queue-based scaling)
    keda:
      pollingInterval: 60     # Longer interval for batch work
      cooldownPeriod: 600     # 10-minute cooldown for compaction
      minReplicaCount: 1
      maxReplicaCount: 5      # Compaction typically needs fewer replicas
      postgresql:
        # Work queue query - counts runnable log compaction jobs
        query: "SELECT COALESCE(COUNT(*), 0) FROM work_queue WHERE needs_run=true AND runnable_at <= now() AND signal='logs' AND action='compact'"
        targetQueryValue: 10
        activationTargetQueryValue: 1
        metricType: "AverageValue"
  terminationGracePeriodSeconds: 300
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Compact Metrics configuration
compactMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 2100m
      memory: 1Gi
    limits:
      cpu: 2100m
      memory: 1Gi
  temporaryStorage:
    size: "5Gi"
  autoscaling:
    enabled: true
    # Scaling mode for this component (overrides global.autoscaling.mode)
    mode: ""
    # HPA configuration
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    # KEDA configuration (work queue-based scaling)
    keda:
      pollingInterval: 60     # Longer interval for batch work
      cooldownPeriod: 600     # 10-minute cooldown for compaction
      minReplicaCount: 1
      maxReplicaCount: 5      # Compaction typically needs fewer replicas
      postgresql:
        # Work queue query - counts runnable metric compaction jobs
        query: "SELECT COALESCE(COUNT(*), 0) FROM work_queue WHERE needs_run=true AND runnable_at <= now() AND signal='metrics' AND action='compact'"
        targetQueryValue: 10
        activationTargetQueryValue: 1
        metricType: "AverageValue"
  terminationGracePeriodSeconds: 300
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Rollup Metrics configuration
rollupMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 1100m
      memory: 1Gi
    limits:
      cpu: 1100m
      memory: 1Gi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    # Scaling mode for this component (overrides global.autoscaling.mode)
    mode: ""
    # HPA configuration
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    # KEDA configuration (work queue-based scaling)
    keda:
      pollingInterval: 60     # Longer interval for batch work
      cooldownPeriod: 600     # 10-minute cooldown for rollup processing
      minReplicaCount: 1
      maxReplicaCount: 8      # Rollup processing can scale higher
      postgresql:
        # Work queue query - counts runnable metric rollup jobs
        query: "SELECT COALESCE(COUNT(*), 0) FROM work_queue WHERE needs_run=true AND runnable_at <= now() AND signal='metrics' AND action='rollup'"
        targetQueryValue: 15
        activationTargetQueryValue: 2
        metricType: "AverageValue"
  terminationGracePeriodSeconds: 300
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Sweeper configuration
sweeper:
  enabled: true
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 100m
      memory: 80Mi
    limits:
      cpu: 250m
      memory: 80Mi
  terminationGracePeriodSeconds: 300
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# PubSub configuration
pubsub:
  HTTP:
    enabled: false
    replicas: 2 # recommend at least 2
    service:
      type: ClusterIP
    image:
      repository: public.ecr.aws/cardinalhq.io/lakerunner
      tag: ""
      pullPolicy: ""
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 200Mi
    terminationGracePeriodSeconds: 120
    nodeSelector: {}
    tolerations: {}
    affinity: {}
  SQS:
    enabled: true
    replicas: 1
    queueURL: "https://sqs.us-east-2.amazonaws.com/123456789012/my-queue"
    region: "us-east-2" # should match the region of the SQS queue
    roleARN: ""
    image:
      repository: public.ecr.aws/cardinalhq.io/lakerunner
      tag: ""
      pullPolicy: ""
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 200Mi
    terminationGracePeriodSeconds: 120
    nodeSelector: {}
    tolerations: {}
    affinity: {}

# Query API configuration
queryApi:
  enabled: true
  replicas: 1
  minWorkers: 2
  maxWorkers: 4
  service:
    type: ClusterIP
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner/query-api
    tag: "latest"
    pullPolicy: ""
  resources:
    requests:
      cpu: 2000m
      memory: 8Gi
    limits:
      cpu: 2000m
      memory: 8Gi
  terminationGracePeriodSeconds: 120
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Query Worker configuration
#
# We do not specify a number of replicas here, as the QueryAPI will scale the number of workers
# based on the number of queries being processed.
queryWorker:
  enabled: true
  # Initial number of replicas for local development only.
  # This is only used when running locally and should be set to 0 for production.
  # The QueryAPI will still manage scaling based on minWorkers and maxWorkers.
  initialReplicas: 0
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner/query-worker
    tag: "latest"
    pullPolicy: ""
  resources:
    requests:
      cpu: 3500m
      memory: 12Gi
    limits:
      cpu: 3500m
      memory: 12Gi
  service:
    type: ClusterIP
  temporaryStorage:
    size: "16Gi"
  terminationGracePeriodSeconds: 120
  nodeSelector: {}
  tolerations: {}
  affinity: {}

# Grafana configuration
grafana:
  enabled: true
  replicas: 1
  image:
    repository: grafana/grafana
    tag: "latest"
    pullPolicy: ""
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
  service:
    type: ClusterIP
    port: 3000
  # CardinalHQ LakeRunner datasource plugin configuration
  # This plugin is REQUIRED for LakeRunner to function properly
  cardinalPlugin:
    # URL to the CardinalHQ LakeRunner datasource plugin
    # Change this only for air-gapped environments where you host the plugin elsewhere
    url: "https://github.com/cardinalhq/cardinalhq-lakerunner-datasource/raw/refs/heads/main/cardinalhq-lakerunner-datasource.zip;cardinalhq-lakerunner-datasource"
  # Additional optional Grafana plugins (semicolon-separated list)
  # These will be installed in addition to the required CardinalHQ plugin
  additionalPlugins: ""
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Cardinal
          type: cardinalhq-lakerunner-datasource
          access: proxy
          isDefault: true
          editable: true
          jsonData:
            # the base URL of the query-api service
            customPath: ""
          secureJsonData:
            # the API key to use for the datasource
            apiKey: ""
