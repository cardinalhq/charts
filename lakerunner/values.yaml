# Default values for lakerunner
# This is a YAML-formatted file.

# Global settings
global:
  # CardinalHQ telemetry configuration
  cardinal:
    # API key for CardinalHQ telemetry endpoint
    # If provided, telemetry will be sent to CardinalHQ's controlled endpoint
    # If not provided, no telemetry endpoint will be configured
    apiKey: ""
  # The names of image pull secrets to use for all images.
  # Optional.
  imagePullSecrets: []
  # Global image settings
  image:
    # Default tag for all lakerunner images (defaults to Chart.appVersion)
    # Override this to use a different tag for all images
    tag: ""
    # Default pull policy for all images
    pullPolicy: "IfNotPresent"
  # Global temporary storage configuration
  # This allows switching between emptyDir and ephemeral volume claim templates
  # Storage sizes are configured per-service in their respective temporaryStorage.size settings
  temporaryStorage:
    # Type of temporary storage: "emptyDir" or "ephemeral"
    # - emptyDir: Traditional emptyDir volumes (default)
    # - ephemeral: Uses ephemeral volume claim templates for CSI-based storage
    type: "emptyDir"
    # Configuration for ephemeral volume claim templates (only used when type: "ephemeral")
    ephemeral:
      # Storage class to use for ephemeral volumes (leave empty for default storage class)
      storageClassName: ""
      # Additional labels to apply to ephemeral volume claim templates
      labels:
        {}
        # type: my-frontend-volume
  # Global autoscaling configuration
  autoscaling:
    # Global scaling mode for all components (can be overridden per component)
    # Options: "keda" (work queue-based), "disabled"
    #
    # PRODUCTION: Use "keda" for production environments.
    # Work queue-based scaling (KEDA) provides intelligent scaling based on actual
    # workload backlog, which is essential for micro-batch workloads.
    #
    # You must have KEDA installed in your cluster to use "keda" mode.
    # See https://keda.sh for installation instructions.
    mode: "disabled"
  # Additional annotations to add to all resources.
  # This is a key: value map.
  # Optional.
  annotations: {}
  # Additional labels to add to all resources.
  # This is a key: value map.
  # Optional.
  labels: {}
  # Common environment variables to inject into all containers.
  # This is a key: value map.
  # Optional.
  env:
    # - name: ENVAR1
    #   value: "value1"
    # - name: ENVAR2
    #   value: "value2"
  # A node selector to apply to all pods.
  # This is standard Kubernetes node selector syntax.
  # Optional.
  nodeSelector:
    {}
    # node-role.kubernetes.io/worker: ""
    # spot-instance: "true"
  # A set of tolerations to apply to all pods.
  # This is standard Kubernetes toleration syntax.
  # Optional.
  tolerations:
    []
    # - key: "spot"
    #   operator: "Equal"
    #   value: "true"
    #   effect: "NoSchedule"
  # A set of affinity rules to apply to all pods.
  # This is standard Kubernetes affinity syntax.
  # Optional.
  affinity:
    {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #       - matchExpressions:
    #           - key: "node-role.kubernetes.io/worker"
    #             operator: In
    #             values:
    #               - "true"
  # The default service account to use for all pods.
  # This service account is assigned some namespaced permissions.
  # Additional cloud-provider specific permissions can be added
  # to this service account, either by annotations or through
  # systems like EKS's IAM roles for service accounts (IRSA).
  # Required.

serviceAccount:
  # Create indicates whether to create the service account.
  # If set to false, the service account must already
  # exist in the Kubernetes cluster.
  create: true
  # The name of the service account to use.
  # If `create` is set to true, this will be the name of the service
  # account created by the Helm chart with the format `<release-name>-lakerunner`.
  # Required.
  name: "lakerunner"
  # The annotations to add to the service account.
  # Only added if create is set to true.
  # This is a key: value map.
  # Optional.
  annotations: {}

# Cloud provider configuration for storage operations
# This section configures authentication for the cloud storage backend used by LakeRunner.
# Choose the primary cloud provider and configure credentials accordingly.
cloudProvider:
  # Primary cloud provider for storage operations
  # Options: "aws", "gcp", "azure"
  # This determines which credentials are injected into all LakeRunner pods.
  # Required.
  provider: "aws" # Change to "azure" or "gcp" based on your storage profiles

  # AWS configuration (used when provider: "aws")
  aws:
    # AWS region for the deployment
    # Required when using AWS provider
    region: ""

    # Name of the Kubernetes secret that contains AWS credentials
    # Secret should contain: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
    secretName: "aws-credentials"

    # Whether to create the AWS credentials secret
    create: false

    # Whether to inject AWS credentials into pods
    # Set to false if using IAM roles (IRSA) or EC2 instance profiles
    inject: true

    # AWS credentials (only used if create: true)
    accessKeyId: ""
    secretAccessKey: ""

  # Azure configuration (used when provider: "azure")
  azure:
    # Name of the Kubernetes secret that contains Azure credentials
    # Secret contents vary based on authType
    secretName: "azure-credentials"

    # Whether to create the Azure credentials secret
    create: false

    # Whether to inject Azure credentials into pods
    # Set to false if using system managed identity without explicit configuration
    inject: true

    # Azure authentication type
    # Options: "service_principal", "user_managed_identity", "system_managed_identity",
    #          "workload_identity", "connection_string"
    authType: "service_principal"

    # Azure service principal credentials (only used if authType: "service_principal")
    clientId: ""
    clientSecret: ""
    tenantId: ""

    # Connection string (only used if authType: "connection_string")
    connectionString: ""

    # Federated token file path (only used if authType: "workload_identity")
    # Default path for Azure Workload Identity webhook
    federatedTokenFile: "/var/run/secrets/azure/tokens/azure-identity-token"

  # GCP configuration (used when provider: "gcp")
  gcp:
    # Name of the Kubernetes secret that contains GCP credentials
    secretName: "gcp-credentials"

    # Whether to create the GCP credentials secret
    create: false

    # Whether to inject GCP credentials into pods
    # Set to false if using Workload Identity or other GCP auth methods
    inject: true

    # GCP credentials (only used if create: true)
    # Both credential types can be provided simultaneously:

    # Service account JSON for native GCP API access (e.g., Pub/Sub)
    serviceAccountJson: ""

    # HMAC keys for S3-compatible GCS access (e.g., storage operations)
    accessKeyId: ""
    secretAccessKey: ""

  # DuckDB configuration (used by query-worker-v2 for S3 access)
  # DuckDB always requires explicit S3 credentials
  duckdb:
    # Name of the Kubernetes secret that contains DuckDB S3 credentials
    # Secret should contain: S3_ACCESS_KEY_ID, S3_SECRET_ACCESS_KEY
    secretName: "duckdb-credentials"

    # Whether to create the DuckDB credentials secret
    create: false

    # S3 credentials for DuckDB (only used if create: true)
    # These can be the same as your cloud provider credentials or separate read-only keys
    accessKeyId: ""
    secretAccessKey: ""

# Kafka topics configuration
# Defines the Kafka topics that will be created during setup.
# Required when using Kafka for data processing.
kafkaTopics:
  source: config # only value for general use.
  configmapName: "kafka-topics" # Name of the ConfigMap containing Kafka topics
  create: true
  yaml:
    - name: "lakerunner.objstore.ingest.logs"
      partitions: 6
      replicationFactor: 3
    - name: "lakerunner.objstore.ingest.metrics"
      partitions: 12
      replicationFactor: 3
    - name: "lakerunner.objstore.ingest.traces"
      partitions: 6
      replicationFactor: 3
    - name: "lakerunner.segments.logs.compact"
      partitions: 3
      replicationFactor: 3
      config:
        cleanup.policy: compact
    - name: "lakerunner.segments.metrics.compact"
      partitions: 3
      replicationFactor: 3
      config:
        cleanup.policy: compact
    - name: "lakerunner.segments.metrics.rollup"
      partitions: 3
      replicationFactor: 3
    - name: "lakerunner.segments.traces.compact"
      partitions: 3
      replicationFactor: 3
      config:
        cleanup.policy: compact

# Storage profile configuration
# At least one storage profile is required for the LakeRunner to function.
# Required.
# If you want to auto create the Cardinal Collector, make sure that your storage profile has the following fields:
# organization_id, collector_name, cloud_provider, region, bucket
storageProfiles:
  source: config # only value for general use.
  configmapName: "storage-profiles" # Name of the ConfigMap containing storage profiles
  create: true
  yaml:
    []
    # - organization_id: dddddddd-aaaa-4ff9-ae8f-365873c552f0
    #   instance_num: 1
    #   collector_name: "kubepi"
    #   cloud_provider: "aws"
    #   region: "us-east-2"
    #   bucket: "datalake-11ndajkhk"
    #   endpoint: ""
    #   use_path_style: true

# API keys are used to control access to the data lake through the data-api service.
# Each key must be unique, and it associates an API key with an organization ID.
# An example format is shown below.  If you wish to create the secret outside of the
# helm chart, follow this format, set `apiKeys.create` to `false`, and create a Kubernetes secret
# with the content in the secret under a key named `apikeys.yaml`.
# Required.
apiKeys:
  source: config # don't change as no other source is supported.
  secretName: "apikeys" # will have the format <release-name>-apikeys once deployed
  create: true
  yaml:
    []
    # - organization_id: dddddddd-aaaa-4ff9-ae8f-365873c552f0
    #   keys:
    #     - my-api-key-1
    #     - my-api-key-2

# Database configuration
# Required.
database:
  secretName: "pg-credentials" # Name of the Kubernetes secret containing database credentials
  create: true # Whether to create the database credentials secret
  # Secret key names (only needed if using an existing secret with different key names)
  passwordKey: "LRDB_PASSWORD" # Key name for password in the secret
  # LRDB (LakeRunner Database) - PostgreSQL
  lrdb:
    # PostgreSQL hostname.
    # Required.
    host: ""
    # PostgreSQL port.  Default is 5432.
    # Required.
    port: 5432
    # PostgreSQL database name.
    # Required.
    name: "lakerunner"
    # PostgreSQL username.
    # Required.
    username: "lakerunner"
    # PostgreSQL password.
    # Optional, but recommended if not using an existing secret.
    password: ""
    # SSL mode for PostgreSQL connection.  Default is "require".
    # Options are "disable", "allow", "prefer", "require", "verify-ca", and "verify-full".
    # See https://www.postgresql.org/docs/current/libpq-ssl.html#LIBPQ-SSL-SSLMODE-STATEMENTS for more details.
    # Optional, but recommended.
    sslMode: "require"

# Configuration database settings
configdb:
  secretName: "configdb-credentials" # Name of the Kubernetes secret containing configdb credentials
  create: true # Whether to create the configdb credentials secret
  # Secret key names (only needed if using an existing secret with different key names)
  passwordKey: "CONFIGDB_PASSWORD" # Key name for password in the secret
  # CONFIGDB (Configuration Database) - PostgreSQL
  lrdb:
    # PostgreSQL hostname.
    # Required.
    host: ""
    # PostgreSQL port.  Default is 5432.
    # Required.
    port: 5432
    # PostgreSQL database name.
    # Required.
    name: "config"
    # PostgreSQL username.
    # Required.
    username: "config"
    # PostgreSQL password.
    # Optional, but recommended if not using an existing secret.
    password: ""
    # SSL mode for PostgreSQL connection.  Default is "require".
    # Options are "disable", "allow", "prefer", "require", "verify-ca", and "verify-full".
    # See https://www.postgresql.org/docs/current/libpq-ssl.html#LIBPQ-SSL-SSLMODE-STATEMENTS for more details.
    # Optional, but recommended.
    sslMode: "require"

# Authentication configuration
auth:
  # token is used for the query-api pod to authenticate to the query-worker pods.
  # This secret must contain a key named `TOKEN` with the value being the token string.
  # Required.
  token:
    # The name of the Kubernetes secret that contains the token.
    # Required.
    secretName: "query-token"
    # If create is set to true, the secret will be created with the secretValue provided below.
    create: true
    secretValue: ""

# Setup job configuration (runs before all other services)
# This job is responsible for running database migrations and initial setup tasks.
setup:
  enabled: true
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 1100m
      memory: 250Mi
    limits:
      cpu: 1100m
      memory: 250Mi
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

  # Kafka topic setup configuration
  # This enables the setup job to create Kafka topics from the kafkaTopics configuration
  kafka:
    enabled: true

# Ingest Logs configuration
ingestLogs:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 2
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 500m
      memory: 200Mi
    limits:
      cpu: 1100m
      memory: 200Mi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 30 # How often to check the queue (seconds)
      cooldownPeriod: 300 # Wait time before scaling down (seconds)
      targetQueueDepth: 100 # Scale up when queue depth exceeds this (logs process quickly)
      activationQueueDepth: 10 # Start scaling when queue depth reaches this
  terminationGracePeriodSeconds: 600
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Ingest Metrics configuration
ingestMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 2
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 1100m
      memory: 500Mi
    limits:
      cpu: 1100m
      memory: 500Mi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 30 # How often to check the queue (seconds)
      cooldownPeriod: 300 # Wait time before scaling down (seconds)
      targetQueueDepth: 150 # Higher threshold for metrics (more resource intensive)
      activationQueueDepth: 20 # Start scaling when queue depth reaches this
  terminationGracePeriodSeconds: 600
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Ingest Traces configuration
ingestTraces:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 2
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 800m
      memory: 400Mi
    limits:
      cpu: 1100m
      memory: 400Mi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 30 # How often to check the queue (seconds)
      cooldownPeriod: 300 # Wait time before scaling down (seconds)
      targetQueueDepth: 120 # Scale up when queue depth exceeds this (traces process moderately)
      activationQueueDepth: 15 # Start scaling when queue depth reaches this
  terminationGracePeriodSeconds: 600
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Compact Logs configuration
compactLogs:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 1100m
      memory: 500Mi
    limits:
      cpu: 1100m
      memory: 500Mi
  temporaryStorage:
    size: "5Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 60 # Longer interval for batch work
      cooldownPeriod: 600 # 10-minute cooldown for compaction
      targetQueueDepth: 10 # Scale up when pending jobs exceed this
      activationQueueDepth: 1 # Start scaling when any jobs are pending
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Compact Metrics configuration
compactMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 2100m
      memory: 1Gi
    limits:
      cpu: 2100m
      memory: 1Gi
  temporaryStorage:
    size: "5Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 60 # Longer interval for batch work
      cooldownPeriod: 600 # 10-minute cooldown for compaction
      targetQueueDepth: 10 # Scale up when pending jobs exceed this
      activationQueueDepth: 1 # Start scaling when any jobs are pending
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Compact Traces configuration
compactTraces:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 1800m
      memory: 1Gi
    limits:
      cpu: 1800m
      memory: 1Gi
  temporaryStorage:
    size: "5Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 60 # Longer interval for batch work
      cooldownPeriod: 600 # 10-minute cooldown for compaction
      targetQueueDepth: 10 # Scale up when pending jobs exceed this
      activationQueueDepth: 1 # Start scaling when any jobs are pending
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Rollup Metrics configuration
rollupMetrics:
  enabled: true
  # The number of replicas to run.  This is not used if autoscaling is enabled.
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 1100m
      memory: 1Gi
    limits:
      cpu: 1100m
      memory: 1Gi
  temporaryStorage:
    size: "10Gi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    keda:
      pollingInterval: 60 # Longer interval for batch work
      cooldownPeriod: 600 # 10-minute cooldown for rollup processing
      targetQueueDepth: 15 # Scale up when pending jobs exceed this
      activationQueueDepth: 2 # Start scaling when jobs are pending
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Sweeper configuration
sweeper:
  enabled: true
  replicas: 1
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 100m
      memory: 80Mi
    limits:
      cpu: 250m
      memory: 80Mi
  terminationGracePeriodSeconds: 300
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Monitoring service configuration
monitoring:
  enabled: true
  replicas: 1
  grpcPort: 9090
  httpPort: 8090
  service:
    type: ClusterIP
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 100m
      memory: 80Mi
    limits:
      cpu: 250m
      memory: 80Mi
  terminationGracePeriodSeconds: 30
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# PubSub configuration
pubsub:
  HTTP:
    enabled: false
    replicas: 2 # recommend at least 2
    service:
      type: ClusterIP
    image:
      repository: public.ecr.aws/cardinalhq.io/lakerunner
      tag: ""
      pullPolicy: ""
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 200Mi
    terminationGracePeriodSeconds: 120
    nodeSelector: {}
    tolerations: []
    affinity: {}
  SQS:
    enabled: false
    replicas: 1
    # REQUIRED when enabled: Your SQS queue URL
    # Example: "https://sqs.us-east-2.amazonaws.com/123456789012/my-queue"
    queueURL: ""
    # AWS region for the SQS queue - defaults to global aws.region if not specified
    # Example: "us-east-2"
    region: ""
    roleARN: ""
    image:
      repository: public.ecr.aws/cardinalhq.io/lakerunner
      tag: ""
      pullPolicy: ""
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 200Mi
    terminationGracePeriodSeconds: 120
    nodeSelector: {}
    tolerations: []
    affinity: {}
  GCP:
    enabled: false
    replicas: 1
    # REQUIRED when enabled: Your GCP project ID
    # Example: "my-project-123456"
    projectID: ""
    # REQUIRED when enabled: Your GCP Pub/Sub subscription ID
    # Example: "my-subscription"
    subscriptionID: ""
    image:
      repository: public.ecr.aws/cardinalhq.io/lakerunner
      tag: ""
      pullPolicy: ""
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 200Mi
    terminationGracePeriodSeconds: 120
    nodeSelector: {}
    tolerations: []
    affinity: {}
    # Component-specific environment variables.
    # This is a list of key-value pairs.
    # Optional.
    env: []
  Azure:
    enabled: false
    replicas: 1
    # REQUIRED when enabled: Your Azure Storage Account name
    # Example: "mystorageaccount"
    storageAccount: ""
    # REQUIRED when enabled: Your Azure Storage Queue name
    # Example: "sqs"
    queueName: ""
    image:
      repository: public.ecr.aws/cardinalhq.io/lakerunner
      tag: ""
      pullPolicy: ""
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 200Mi
    terminationGracePeriodSeconds: 120
    nodeSelector: {}
    tolerations: []
    affinity: {}
    # Component-specific environment variables.
    # This is a list of key-value pairs.
    # Optional.
    env: []

# Query API v2 configuration
queryApiV2:
  enabled: true
  replicas: 2
  service:
    type: ClusterIP
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 2000m
      memory: 2Gi
    limits:
      cpu: 2000m
      memory: 2Gi
  temporaryStorage:
    size: "16Gi"
  terminationGracePeriodSeconds: 120
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Query Worker v2 configuration
queryWorkerV2:
  enabled: true
  replicas: 5
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner
    tag: ""
    pullPolicy: ""
  resources:
    requests:
      cpu: 2000m
      memory: 6Gi
    limits:
      cpu: 2000m
      memory: 6Gi
  service:
    type: ClusterIP
  temporaryStorage:
    size: "16Gi"
  terminationGracePeriodSeconds: 120
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Query API v1 (legacy) configuration - DEPRECATED
queryApi:
  enabled: false
  replicas: 1
  minWorkers: 8
  maxWorkers: 8
  service:
    type: ClusterIP
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner/query-api
    tag: "v1.2.1"
    pullPolicy: ""
  resources:
    requests:
      cpu: 2000m
      memory: 8Gi
    limits:
      cpu: 2000m
      memory: 8Gi
  terminationGracePeriodSeconds: 120
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Query Worker v1 (legacy) configuration - DEPRECATED
#
# We do not specify a number of replicas here, as the QueryAPI will scale the number of workers
# based on the number of queries being processed.
queryWorker:
  enabled: false
  # Initial number of replicas for local development only.
  # This is only used when running locally and should be set to 0 for production.
  # The QueryAPI will still manage scaling based on minWorkers and maxWorkers.
  initialReplicas: 0
  image:
    repository: public.ecr.aws/cardinalhq.io/lakerunner/query-worker
    tag: "v1.2.1"
    pullPolicy: ""
  resources:
    requests:
      cpu: 3500m
      memory: 12Gi
    limits:
      cpu: 3500m
      memory: 12Gi
  service:
    type: ClusterIP
  temporaryStorage:
    size: "16Gi"
  terminationGracePeriodSeconds: 120
  labels: {}
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Grafana configuration
grafana:
  enabled: true
  replicas: 1 # For replicas > 1, configure external database via grafana.env (see Grafana docs)
  # CardinalHQ LakeRunner datasource configuration
  # This provides a simplified way to configure the Cardinal datasource
  # Required.
  cardinal:
    # REQUIRED: API key for datasource authentication
    # Must match a key from apiKeys.yaml configuration
    apiKey: ""
    # Optional: Custom endpoint URL for the Cardinal datasource
    # If not specified, auto-configured to point to the deployed query-api service
    # Example: "http://my-custom-query-api:8080"
    endpoint: ""
    # Optional: Custom name for the datasource (defaults to "Cardinal")
    name: "Cardinal"
    # Optional: Whether this datasource should be the default (defaults to true)
    isDefault: true
    # Optional: Whether this datasource should be editable in Grafana UI (defaults to true)
    editable: true
  image:
    repository: grafana/grafana
    tag: "latest"
    pullPolicy: ""
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
  service:
    type: ClusterIP
    port: 3000
  # CardinalHQ LakeRunner datasource plugin configuration
  # This plugin is REQUIRED for LakeRunner to function properly
  cardinalPlugin:
    # URL to the CardinalHQ LakeRunner datasource plugin
    # air-gapped deployments should download the plugin and provide a local path
    url: "https://github.com/cardinalhq/cardinalhq-lakerunner-datasource/releases/download/v1.2.0-rc.3/cardinalhq-lakerunner-datasource.zip;cardinalhq-lakerunner-datasource"
  # Additional optional Grafana plugins (semicolon-separated list)
  # These will be installed in addition to the required CardinalHQ plugin
  additionalPlugins: ""
  # Additional datasource configurations
  # Use this to add extra datasources beyond the Cardinal datasource
  # Each key becomes a filename in the datasources provisioning directory
  # Optional.
  datasources:
    {}
    # example-datasource.yaml:
    #   apiVersion: 1
    #   datasources:
    #     - name: Prometheus
    #       type: prometheus
    #       url: http://prometheus:9090

# Collector configuration
# This section controls the CardinalHQ Collector resource creation
collector:
  # Whether to create the Collector resource
  # Default: true (creates collector when Cardinal API key and storage profiles are configured)
  enabled: true

  # Additional environment variables to inject into the Collector
  # These will be merged with the default environment variables
  # Optional.
  env:
    # - name: CUSTOM_ENV_VAR
    #   value: "custom-value"
    # - name: ANOTHER_VAR
    #   value: "another-value"
    # - name: SECRET_VAR
    #   valueFrom:
    #     secretKeyRef:
    #       name: my-secret
    #       key: my-key

  # Additional labels to apply to the Collector resource
  # These will be merged with the default labels
  # Optional.
  labels: {}

  # Additional annotations to apply to the Collector resource
  # These will be merged with the default annotations
  # Optional.
  annotations: {}
